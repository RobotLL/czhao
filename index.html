<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <link
        href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap"
        rel="stylesheet" />
    <link rel="stylesheet" type="text/css" href="assets/css/style.css" />
    <title>Chao Zhao</title>
    <link rel="icon" type="image/x-icon" href="assets/images/seas.ico" />
</head>

<body>
    <div style="width:1000px; margin: 0 auto;">
        <header id="header" style="display: flex; justify-content: space-around;">
            <a href="#profile-intro">Home</a>
            <a href="#updates">Updates</a>
            <a href="#research">Research</a>
            <a href="#updates">Teaching & Outreach</a>
        </header>

        <div id="profile">
            <div id="profile-pic" style="display: flex; flex-direction: column; align-items: center;">
                <img src="assets/images/profile.jpg" alt="Profile picture" />
                <p style="margin-top: 5px;">
                    <a href="https://scholar.google.com/citations?user=XJkLIPQAAAAJ&hl=zh-CN">Google Scholar</a>
                    /
                    <a href="mailto:chaozhao@jlu.edu.cn">Email</a>
                </p>
            </div>
            <div id="profile-intro">
                <div id="profile-name">Chao Zhao</div>
                <div id="profile-email">chaozhao at jlu dot edu dot cn</div>
                <p>
                    I am currently an Associate Professor at the School of Artificial Intelligence, Jilin University.
                    I received my Ph.D. from HKUST in 2025 (advisor: Prof.
                    <a href="https://cqf.io//">Qifeng Chen</a>) and my Master’s from the University of Birmingham
                    (advisor: Prof.
                    <a href="https://scholar.google.com/citations?user=9FSuHUQAAAAJ&hl=en//">Jeremy Wyatt</a>).
                </p>
                <p>
                    My research focuses on developing algorithms and systems that endow robots with intelligence,
                    enabling them to perceive, reason, act, and assist humans effectively in real-world
                    environments. My interests span dexterous hands, tactile multimodality, as well as long-term
                    autonomy in humanoid robots.
                </p>
                <p>
                    <strong>Openings:</strong> I will have one Ph.D. position and 2–3 Master's openings each year.
                    Feel free to reach out if you're interested!
                </p>
                <p>
                    <strong>招生信息：</strong> 我每年招收一名博士生和两到三名硕士生，欢迎感兴趣的同学联系我！
                </p>
            </div>
            <div style="clear: both;"></div>
        </div>

        <div class="section" id="updates">
            <h1>Updates</h1>
            <ul>
                <li><b>June 2025</b> Our paper <a href="https://robotll.github.io/LTDOM/">LTDOM</a> is accepted by
                    <a href="https://arxiv.org/abs/2411.13952">T-RO</a>!
                <li><b>Jan 2025</b> Our paper
                    <a href="https://robotll.github.io/MasterRulesFromChaos/">MrChaos</a> is accepted by
                    <a href="https://arxiv.org/abs/2505.11818">ICRA 2025</a>!
                <li><b>Apr 2023</b> Our paper <a href="https://robotll.github.io/ERRA/">ERRA</a> is accepted by
                    <a href="https://arxiv.org/abs/2304.02251">RA-L</a>!
                <li><b>Jan 2023</b> Our paper <a href="https://arxiv.org/abs/2304.02253">Flipbot</a> is accepted by
                    <a href="https://arxiv.org/abs/2304.02253">ICRA 2023</a>!
                <li><b>Dec 2022</b> Our paper
                    <a href="https://robotll.github.io/LearnfromIntents/">Learn to Grasp via Intention Discovery</a> is
                    accepted by <a href="https://arxiv.org/abs/2304.02252">RA-L</a>!
            </ul>
            <div style="clear: both;"></div>
        </div>

        <div class="divider"></div>

        <div id="research">
            <h1>Research</h1>
            <p>* indicates equal contribution</p>

            <!-- Entry 1 -->
            <div class="research-entry">
                <a href="https://robotll.github.io/MasterRulesFromChaos/" class="research-thumb">
                    <img src="assets/images/mrchaos.jpg" alt="Master Rules from Chaos thumbnail" />
                </a>
                <div class="research-detail">
                    <a href="https://robotll.github.io/MasterRulesFromChaos/" class="research-proj-title">
                        Master Rules from Chaos: Learning to Reason, Plan, and Interact from Chaos for Tangram Assembly
                    </a>
                    <p class="research-authors">
                        <b>Chao Zhao*</b>, Chunli Jiang*, Lifan Luo, Guanlan Zhang, Hongyu Yu, Michael Yu Wang, Qifeng
                        Chen
                    </p>
                    <p class="research-venue">
                        IEEE International Conference on Robotics and Automation (ICRA) 2025
                    </p>
                    <p class="research-links">
                        <a href="https://robotll.github.io/MasterRulesFromChaos/">Website</a> •
                        <a href="https://arxiv.org/abs/2505.11818">ArXiv</a> •
                        <a href="https://robotll.github.io/MasterRulesFromChaos/">Code</a>
                    </p>
                </div>
            </div>

            <!-- Entry 2 -->
            <div class="research-entry">
                <a href="https://robotll.github.io/LTDOM/" class="research-thumb">
                    <img src="assets/images/ltdom.jpg" alt="LTDOM thumbnail" />
                </a>
                <div class="research-detail">
                    <a href="https://robotll.github.io/LTDOM/" class="research-proj-title">
                        Learning thin deformable object manipulation with embodied multisensory integration
                    </a>
                    <p class="research-authors">
                        <b>Chao Zhao*</b>, Chunli Jiang*, Lifan Luo*, Shuai Yuan, Qifeng Chen, Hongyu Yu
                    </p>
                    <p class="research-venue">
                        IEEE Transactions on Robotics (T-RO)
                    </p>
                    <p class="research-links">
                        <a href="https://robotll.github.io/LTDOM/">Website</a> •
                        <a href="https://arxiv.org/abs/2411.13952">ArXiv</a> •
                        <a href="https://robotll.github.io/LTDOM/">Code</a>
                    </p>
                </div>
            </div>

            <!-- Entry 3 -->
            <div class="research-entry">
                <a href="https://robotll.github.io/ERRA/" class="research-thumb">
                    <img src="assets/images/erra.gif" alt="ERRA thumbnail" />
                </a>
                <div class="research-detail">
                    <a href="https://robotll.github.io/ERRA/" class="research-proj-title">
                        ERRA: An Embodied Representation and Reasoning Architecture for Long-horizon
                        Language-conditioned Manipulation Tasks
                    </a>
                    <p class="research-authors">
                        <b>Chao Zhao*</b>, Shuai Yuan*, Chunli Jiang, Junhao Cai, Hongyu Yu, Michael Yu Wang, Qifeng
                        Chen
                    </p>
                    <p class="research-venue">
                        IEEE Robotics and Automation Letters (RA-L) 2023
                    </p>
                    <p class="research-links">
                        <a href="https://robotll.github.io/ERRA/">Website</a> •
                        <a href="https://arxiv.org/abs/2304.02251">ArXiv</a> •
                        <a href="https://robotll.github.io/ERRA/">Video</a> •
                        <a href="https://robotll.github.io/ERRA/">Code</a>
                    </p>
                </div>
            </div>

            <!-- Entry 4 -->
            <div class="research-entry">
                <a href="https://robotll.github.io/Flipbot/" class="research-thumb">
                    <img src="assets/images/flip.gif" alt="Flipbot thumbnail" />
                </a>
                <div class="research-detail">
                    <a href="https://arxiv.org/abs/2304.02253" class="research-proj-title">
                        Flipbot: Learn Continuous Paper Flipping via Coarse-to-Fine Exteroceptive-Proprioceptive Exploration
                    </a>
                    <p class="research-authors">
                        <b>Chao Zhao*</b>, Chunli Jiang*, Junhao Cai, Hongyu Yu, Michael Yu Wang, Qifeng Chen
                    </p>
                    <p class="research-venue">
                        IEEE International Conference on Robotics and Automation (ICRA) 2023
                    </p>
                    <p class="research-links">
                        <a href="https://robotll.github.io/Flipbot/">Website</a> •
                        <a href="https://arxiv.org/abs/2304.02253">ArXiv</a> •
                        <a href="https://robotll.github.io/Flipbot/">Video</a> •
                        <a href="https://robotll.github.io/Flipbot/">Code</a>
                    </p>
                </div>
            </div>

            <!-- Entry 5 -->
            <div class="research-entry">
                <a href="https://robotll.github.io/LearnfromIntents/" class="research-thumb">
                    <img src="assets/images/lin.jpg" alt="Learn to Grasp via Intents thumbnail" />
                </a>
                <div class="research-detail">
                    <a href="https://arxiv.org/abs/2304.02252" class="research-proj-title">
                        Learn to Grasp via Intention Discovery and its Application to Challenging Clutter
                    </a>
                    <p class="research-authors">
                        <b>Chao Zhao</b>, Chunli Jiang, Junhao Cai, Hongyu Yu, Michael Yu Wang, Qifeng Chen
                    </p>
                    <p class="research-venue">
                        IEEE Robotics and Automation Letters (RA-L) 2023
                    </p>
                    <p class="research-links">
                        <a href="https://robotll.github.io/LearnfromIntents/">Website</a> •
                        <a href="https://arxiv.org/abs/2304.02252">ArXiv</a> •
                        <a href="https://robotll.github.io/LearnfromIntents/">Video</a> •
                        <a href="https://robotll.github.io/LearnfromIntents/">Code</a>
                    </p>
                </div>
            </div>

            <!-- Entry 6 -->
            <div class="research-entry">
                <a href="https://ieeexplore.ieee.org/abstract/document/9981530/" class="research-thumb">
                    <img src="assets/images/inter.jpg" alt="Learn from Interaction thumbnail" />
                </a>
                <div class="research-detail">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9981530/" class="research-proj-title">
                        Learn from Interaction: Learning to Pick via Reinforcement Learning in Challenging
                        Clutter
                    </a>
                    <p class="research-authors">
                        <b>Chao Zhao</b>, Jungwon Seo
                    </p>
                    <p class="research-venue">
                        IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2022
                    </p>
                    <p class="research-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/9981530/">Paper</a>
                    </p>
                </div>
            </div>

            <!-- Entry 7 -->
            <div class="research-entry">
                <a href="https://hkust-rml.github.io/Learning-to-Grasp-by-Digging/" class="research-thumb">
                    <img src="assets/images/dig.jpg" alt="Learning to Pick by Digging thumbnail" />
                </a>
                <div class="research-detail">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9811736" class="research-proj-title">
                        Learning to Pick by Digging: Data-Driven Dig-Grasping for Bin Picking from Clutter
                    </a>
                    <p class="research-authors">
                        <b>Chao Zhao*</b>, Zhekai Tong*, Juan Rojas, Jungwon Seo
                    </p>
                    <p class="research-venue">
                        IEEE International Conference on Robotics and Automation (ICRA) 2022
                    </p>
                    <p class="research-links">
                        <a href="https://hkust-rml.github.io/Learning-to-Grasp-by-Digging/">Website</a> •
                        <a href="https://ieeexplore.ieee.org/abstract/document/9811736">ArXiv</a> •
                        <a href="https://hkust-rml.github.io/Learning-to-Grasp-by-Digging/">Video</a>
                    </p>
                </div>
            </div>

            <!-- Entry 8 -->  
            <div class="research-entry">
                <a href="https://arxiv.org/abs/1908.04293" class="research-thumb">
                    <img src="assets/images/dex.jpg" alt="Deep dexterous grasping thumbnail" />
                </a>
                <div class="research-detail">
                    <a href="https://arxiv.org/abs/1908.04293" class="research-proj-title">
                        Deep dexterous grasping of novel objects from a single view
                    </a>
                    <p class="research-authors">
                        Umit Rusen Aktas, <b>Chao Zhao</b>, Marek Kopicki, Ales Leonardis, Jeremy L. Wyatt
                    </p>
                    <p class="research-venue">
                        International Journal of Humanoid Robotics 2022
                    </p>
                    <p class="research-links">
                        <a href="https://rusen.github.io/DDG/">Website</a> •
                        <a href="https://arxiv.org/abs/1908.04293">ArXiv</a>
                    </p>
                </div>
            </div>

        </div>

        <div class="divider"></div>

        <div class="section" id="updates">
            <h1>Teaching & Outreach</h1>
            <ul>
                <li><b>Fall 2021</b> COMP4471: Deep Learning in Computer Vision
                <li><b>Spring 2021</b> IOTA5101: Fog/Edge/Cloud Computing for IoT
            </ul>
            <div style="clear: both;"></div>
        </div>
    </div>
</body>

</html>
